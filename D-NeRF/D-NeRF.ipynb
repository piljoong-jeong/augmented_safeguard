{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# NOTE: material: https://drive.google.com/drive/u/0/folders/14J6oWZzULqtXfIv99064Cs0iHVdeat22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: access to directory\n",
    "dir_project = os.path.abspath(\".\")\n",
    "print(f\"{dir_project=}\")\n",
    "os.chdir(dir_project)\n",
    "print(f\"{os.getcwd()=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "import math\n",
    "import time\n",
    "\n",
    "import cv2\n",
    "import imageio\n",
    "import lpips\n",
    "import numpy as np\n",
    "import torch\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import mcubes\n",
    "import trimesh\n",
    "from tqdm import tqdm, trange\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: extended `config_parser()`\n",
    "\n",
    "import NeRF.config_parser\n",
    "\n",
    "def config_parser():\n",
    "    import argparse\n",
    "    parser = NeRF.config_parser.config_parser()\n",
    "\n",
    "    # NOTE: D-NeRF training options\n",
    "    # ------------------------------------------\n",
    "    parser.add_argument(\"--nerf_type\", type=str, default=\"original\", help=\"nerf network type\")\n",
    "    parser.add_argument(\"--N_iter\", type=int, default=500000, help=\"num training iterations\")\n",
    "    parser.add_argument(\"--do_half_precision\", action=\"store_true\", help=\"do half precision training and inference\")\n",
    "\n",
    "    parser.add_argument(\"--add_tv_loss\", action=\"store_true\", help=\"evaluate tv loss\")\n",
    "    parser.add_argument(\"--tv_loss_weight\", type=float, default=1.e-4, help=\"weight of tv loss\")\n",
    "    # ------------------------------------------\n",
    "    \n",
    "    # NOTE: D-NeRF rendering options\n",
    "    # ------------------------------------------\n",
    "    parser.add_argument(\"--not_zero_canonical\", action=\"store_true\", help=\"if set zero time is not the canonic space\")\n",
    "    parser.add_argument(\"--use_two_models_for_fine\", action=\"store_true\", help=\"use two models for fine results\")\n",
    "    # ------------------------------------------\n",
    "\n",
    "    return parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: parse arguments & load config\n",
    "parser = config_parser()\n",
    "filename_config = \"configs/mutant.txt\"\n",
    "configs = NeRF.config_parser.load_config(parser, filename_config)\n",
    "print(configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: assign configs to arguments\n",
    "\n",
    "args = parser.parse_args(args=[])\n",
    "args.config = filename_config\n",
    "args.expname = configs['expname']\n",
    "args.basedir = configs['basedir']\n",
    "args.datadir = configs['datadir']\n",
    "args.dataset_type = configs['dataset_type']\n",
    "\n",
    "args.nerf_type = configs['nerf_type']\n",
    "args.no_batching = configs['no_batching']\n",
    "args.not_zero_canonical = configs['not_zero_canonical']\n",
    "\n",
    "args.use_viewdirs = configs['use_viewdirs']\n",
    "args.white_bkgd = configs['white_bkgd']\n",
    "args.lrate_decay = int(configs['lrate_decay'])\n",
    "\n",
    "args.N_iter = int(configs['N_iter'])\n",
    "args.N_samples = int(configs['N_samples'])\n",
    "args.N_importance = int(configs['N_importance'])\n",
    "args.N_rand = int(configs['N_rand'])\n",
    "args.testskip = int(configs['testskip'])\n",
    "\n",
    "args.precrop_iters = int(configs['precrop_iters'])\n",
    "args.precrop_iters_time = int(configs['precrop_iters_time'])\n",
    "args.precrop_frac = float(configs['precrop_frac'])\n",
    "\n",
    "args.half_res = bool(configs['half_res'])\n",
    "args.do_half_precision = configs['do_half_precision']\n",
    "\n",
    "args.no_reload = True # NOTE: for now set to True as we'll implement training code\n",
    "\n",
    "print(args)\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "DEBUG=False\n",
    "print(device)\n",
    "\n",
    "torch.set_default_tensor_type('torch.cuda.FloatTensor')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_datadir = \"./data/mutant/\"\n",
    "\n",
    "with open(os.path.join(_datadir, \"transforms_test.json\"), \"r\") as fp:\n",
    "    meta = json.load(fp)\n",
    "\n",
    "print(str(meta)[:300])\n",
    "\n",
    "_frames = meta['frames']\n",
    "_frame = _frames[0]\n",
    "print(_frame)\n",
    "\n",
    "_fname = os.path.join(_datadir, _frame['file_path'] + \".png\")\n",
    "print(_fname)\n",
    "\n",
    "import imageio.v2 as imageio\n",
    "img = imageio.imread(_fname) # [800, 800, 4 = RGB+mask(synthetic; from Blender)]\n",
    "print(img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: sequential visualization\n",
    "\n",
    "fig, axis = plt.subplots(1, 10, figsize=(30, 300))\n",
    "for i in range(10):\n",
    "    _frame = _frames[i]\n",
    "    _fname = os.path.join(_datadir, _frame[\"file_path\"] + \".png\")\n",
    "\n",
    "    img = imageio.imread(_fname)\n",
    "\n",
    "    axis[i].imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: random shuffled visualization\n",
    "\n",
    "import random\n",
    "random.shuffle(_frames)\n",
    "fig, axis = plt.subplots(1, 10, figsize=(30, 300))\n",
    "for i in range(10):\n",
    "    _frame = _frames[i]\n",
    "    _fname = os.path.join(_datadir, _frame[\"file_path\"] + \".png\")\n",
    "\n",
    "    img = imageio.imread(_fname)\n",
    "\n",
    "    axis[i].imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: decomposed channel visualization\n",
    "\n",
    "# NOTE: blocked here to prevent manipulation of original data\n",
    "if False: \n",
    "    imgs = []\n",
    "    for i in range(10):\n",
    "        _frame = _frames[i]\n",
    "        _fname = os.path.join(_datadir, _frame[\"file_path\"] + \".png\")\n",
    "\n",
    "        img = imageio.imread(_fname)\n",
    "        imgs.append(img)\n",
    "\n",
    "    imgs = (np.array(imgs) / 255.0).astype(np.float32) # [0., 1.]\n",
    "    print(imgs.shape)\n",
    "\n",
    "    imgs_orig = imgs[..., :3] # RGB\n",
    "    imgs_mask = imgs[..., -1] # mask\n",
    "\n",
    "    fig, axis = plt.subplots(1, 3)\n",
    "    axis[0].imshow(imgs_orig[0])\n",
    "    axis[1].imshow(imgs_mask[0], cmap=\"gray\")\n",
    "\n",
    "    # NOTE: how plt operates with mask\n",
    "    imgs = imgs[..., :3] * imgs[..., -1:] + (1-imgs[..., -1:]) # `-1:`: broadcast \n",
    "    axis[2].imshow(imgs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: load blender data\n",
    "\n",
    "import NeRF.dataloader\n",
    "import NeRF.pose\n",
    "import NeRF.util\n",
    "\n",
    "# NOTE: implement Blender data loader\n",
    "def load_blender_data(basedir, half_res: bool=False, testskip=1):\n",
    "\n",
    "    \"\"\"\n",
    "    NOTE: if D-NeRF extended, marked as # as a suffix\n",
    "    \"\"\"\n",
    "\n",
    "    images, poses, render_poses, hwf, i_split = NeRF.dataloader.load_blender_data(basedir, half_res, testskip)\n",
    "\n",
    "    splits = [\"train\", \"val\", \"test\"]\n",
    "    metas = {}\n",
    "\n",
    "    # NOTE: load poses\n",
    "    for s in splits:\n",
    "        with open(os.path.join(basedir, f\"transforms_{s}.json\"), \"r\") as fp:\n",
    "            metas[s] = json.load(fp)\n",
    "\n",
    "    all_times = [] #\n",
    "\n",
    "    for s in splits:\n",
    "        meta = metas[s]\n",
    "        times = [] # \n",
    "        skip = testskip # # NOTE: `time` should be re-normalized based on this\n",
    "\n",
    "        for t, frame in enumerate(meta['frames'][::skip]):\n",
    "            times.append(\n",
    "                (cur_time := frame['time'] if 'time' in frame else float(t) / (len(meta['frames'][::skip]) - 1))\n",
    "            ) #\n",
    "\n",
    "        assert times[0] == 0, \"Time must start at 0\" # \n",
    "\n",
    "        times = np.array(times).astype(np.float32) #\n",
    "        all_times.append(times) #\n",
    "\n",
    "    times = np.concatenate(all_times, 0) #\n",
    "\n",
    "    # assert poses.shape == times.shape\n",
    "\n",
    "    path_transforms_render = os.path.join(basedir, \"transforms_render.json\")\n",
    "    if os.path.exists(path_transforms_render):\n",
    "        \n",
    "        with open(path_transforms_render, \"r\") as fp:\n",
    "            meta = json.load(fp)\n",
    "        \n",
    "        render_poses = []\n",
    "        for frame in meta[\"frames\"]:\n",
    "            render_poses.append(np.array(frame[\"transform_matrix\"]))\n",
    "        render_poses = np.array(render_poses).astype(np.float32)\n",
    "\n",
    "    else:\n",
    "        render_poses = torch.stack(\n",
    "            [\n",
    "                NeRF.pose.pose_spherical(theta=angle, phi=-30.0, radius=4.0) \n",
    "                for angle\n",
    "                in np.linspace(-180, 180, 40+1)[:-1]\n",
    "            ], dim=0\n",
    "        )\n",
    "    render_times = torch.linspace(0.0, 1.0, render_poses.shape[0])\n",
    "\n",
    "    return images, poses, times, render_poses, render_times, hwf, i_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: validation: load_blender_data\n",
    "images, poses, times, render_poses, render_times, hwf, i_split = load_blender_data(args.datadir, args.half_res, args.testskip)\n",
    "\n",
    "# print(f\"{i_split=}\")\n",
    "# print(f\"{times=}\")\n",
    "\n",
    "\n",
    "print(f\"{images.shape=}\")\n",
    "print(f\"{poses.shape=}\")\n",
    "print(f\"{times.shape=}\")\n",
    "\n",
    "print(poses[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: positional encoding\n",
    "\n",
    "import NeRF.embedder\n",
    "\n",
    "def get_embedder(multires, input_dims, i=0):\n",
    "    if i == -1:\n",
    "        return nn.Identity(), input_dims\n",
    "    \n",
    "    embed_kwargs = {\n",
    "                'include_input' : True,\n",
    "                'input_dims' : input_dims,\n",
    "                'max_freq_log2' : multires-1,\n",
    "                'num_freqs' : multires,\n",
    "                'log_sampling' : True,\n",
    "                'periodic_funcs' : [torch.sin, torch.cos],\n",
    "    }\n",
    "\n",
    "    embedder_obj = NeRF.embedder.Embedder(**embed_kwargs)\n",
    "    embed = lambda x, eo=embedder_obj : eo.embed(x)\n",
    "\n",
    "    return embed, embedder_obj.out_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: model\n",
    "import torch.nn as nn\n",
    "\n",
    "import NeRF.model\n",
    "\n",
    "class NeRFOriginal(NeRF.model.NeRF):\n",
    "    \"\"\"\n",
    "    \n",
    "    Canonical network\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self, \n",
    "            D=8, \n",
    "            W=256, \n",
    "            input_ch=3, \n",
    "            input_ch_views=3, \n",
    "            output_ch=4, \n",
    "            skips=[4], \n",
    "            use_viewdirs=False, \n",
    "\n",
    "            input_ch_time=1,\n",
    "            memory=[], \n",
    "            embed_fn=None, \n",
    "            output_color_ch=3, \n",
    "            zero_canonical=True) -> None:\n",
    "        super(NeRFOriginal, self).__init__(D, W, input_ch, input_ch_views, output_ch, skips, use_viewdirs)\n",
    "\n",
    "        layers = [nn.Linear(input_ch, W)]\n",
    "        for i in range(D-1):\n",
    "            layer = nn.Linear\n",
    "\n",
    "            in_channels = W\n",
    "            if i in skips:\n",
    "                in_channels += input_ch\n",
    "\n",
    "            layers += [layer(in_channels, W)]\n",
    "\n",
    "        self.pts_linears = nn.ModuleList(layers)\n",
    "        self.views_linears = nn.ModuleList([nn.Linear(input_ch_views+W, W//2)])\n",
    "\n",
    "        if use_viewdirs:\n",
    "            self.feature_linear = nn.Linear(W, W)\n",
    "            self.alpha_linear = nn.Linear(W, 1)\n",
    "            self.rgb_linear = nn.Linear(W//2, output_color_ch)\n",
    "        else:\n",
    "            # NOTE: unused?\n",
    "            self.output_linear = nn.Linear(W, output_ch)\n",
    "\n",
    "    def forward(self, x, ts):\n",
    "\n",
    "        input_pts, input_views = torch.split(\n",
    "            x, \n",
    "            [self.input_ch, self.input_ch_views], dim=-1\n",
    "        )\n",
    "        h = input_pts\n",
    "\n",
    "        for idx, _ in enumerate(self.pts_linears):\n",
    "            h = self.pts_linears[idx](h)\n",
    "            h = F.relu(h)\n",
    "\n",
    "            if idx in self.skips:\n",
    "                h = torch.cat([input_pts, h], dim=-1)\n",
    "\n",
    "        if self.use_viewdirs:\n",
    "            alpha = self.alpha_linear(h)\n",
    "            feature = self.feature_linear(h)\n",
    "            h = torch.cat([feature, input_views], dim=-1)\n",
    "\n",
    "            for idx, _ in enumerate(self.views_linears):\n",
    "                h = self.views_linears[idx](h)\n",
    "                h = F.relu(h)\n",
    "\n",
    "            rgb = self.rgb_linear(h)\n",
    "            outputs = torch.cat([rgb, alpha], dim=-1)\n",
    "\n",
    "        else:\n",
    "            outputs = self.output_linear(h)\n",
    "\n",
    "        return outputs, torch.zeros_like(input_pts[:, :3]) # NOTE: for compatibility with Deformation network\n",
    "\n",
    "\n",
    "# NOTE: here both Deformation network and Canonical network is implemented\n",
    "class DirectTemporalNeRF(nn.Module):\n",
    "    def __init__(self, \n",
    "                 D=8, \n",
    "                 W=256, \n",
    "                 input_ch=3,\n",
    "                 input_ch_views=3, \n",
    "                 input_ch_time=1, \n",
    "                 output_ch=4, \n",
    "                 skips=[4], \n",
    "                 use_viewdirs=False, \n",
    "                 memory=[],\n",
    "                 embed_fn=None, \n",
    "                 zero_canonical=True\n",
    "                 ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # NOTE: from vanilla NeRF\n",
    "        self.D = D\n",
    "        self.W = W\n",
    "        self.input_ch = input_ch\n",
    "        self.input_ch_views = input_ch_views\n",
    "        self.skips = skips\n",
    "        self.use_viewdirs = use_viewdirs\n",
    "\n",
    "        # NOTE: D-NeRF\n",
    "        self.input_ch_time = input_ch_time\n",
    "        self.memory = memory\n",
    "        self.embed_fn = embed_fn\n",
    "        self.zero_canonical = zero_canonical\n",
    "\n",
    "        # NOTE: Canonical network is vanilla NeRF\n",
    "        self._occ = NeRFOriginal(\n",
    "            D=D, \n",
    "            W=W, \n",
    "            input_ch=input_ch,\n",
    "            input_ch_views=input_ch_views,\n",
    "            input_ch_time=input_ch_time,\n",
    "            output_ch=output_ch,\n",
    "            skips=skips,\n",
    "            use_viewdirs=use_viewdirs,\n",
    "            memory=memory,\n",
    "            embed_fn=embed_fn,\n",
    "            output_color_ch=3\n",
    "        )\n",
    "\n",
    "        # NOTE: Deformation network\n",
    "        self._time, self._time_out = self.create_time_net()\n",
    "\n",
    "        return\n",
    "    \n",
    "    def create_time_net(self):\n",
    "        \"\"\"\n",
    "        ### DirectTemporalNeRF.create_time_net()\n",
    "\n",
    "        Encoded (position, time) => delta transformation from Canonical space\n",
    "        \"\"\"\n",
    "\n",
    "        layers = [\n",
    "            nn.Linear(self.input_ch + self.input_ch_time, self.W)\n",
    "        ]\n",
    "        for i in range(self.D - 1):\n",
    "            layer = nn.Linear\n",
    "\n",
    "            in_channels = self.W\n",
    "            if i in self.skips:\n",
    "                in_channels += self.input_ch # NOTE: skip connection\n",
    "            \n",
    "            layers += [layer(in_channels, self.W)]\n",
    "\n",
    "        return (deformation_network := nn.ModuleList(layers)), (final_deformation_layer := nn.Linear(self.W, 3))\n",
    "    \n",
    "    def query_time(self, new_pts, t, net, net_final):\n",
    "        \"\"\"\n",
    "        ### DirectTemporalNeRF.query_time\n",
    "\n",
    "        Inference Deformation network\n",
    "\n",
    "        Encoded (position, time) => delta transformation from Canonical space\n",
    "\n",
    "        Arguments:\n",
    "        - new_pts: chunked batch samples\n",
    "        - t: time, dimension is same as `new_pts`\n",
    "        \"\"\"\n",
    "\n",
    "        h = torch.cat([new_pts, t], dim=-1)\n",
    "\n",
    "        for i, l in enumerate(net):\n",
    "            h = net[i](h)\n",
    "            h = F.relu(h)\n",
    "\n",
    "            if i in self.skips:\n",
    "                h = torch.cat([new_pts, h], dim=-1)\n",
    "\n",
    "        # NOTE: final layer\n",
    "        return net_final(h)\n",
    "\n",
    "    def forward(self, x, ts):\n",
    "\n",
    "        input_pts, input_views = torch.split(\n",
    "            x, \n",
    "            [self.input_ch, self.input_ch_views], \n",
    "            dim=-1\n",
    "        )\n",
    "        t = ts[0]\n",
    "        assert len(torch.unique(t[:, :1])) == 1, \"Only accepts all points from same time\"\n",
    "\n",
    "        cur_time = t[0, 0] # NOTE: any index have equivalent value\n",
    "        if cur_time == 0.0 and self.zero_canonical:\n",
    "            dx = torch.zeros_like(input_pts[:, :3])\n",
    "        else:\n",
    "            dx = self.query_time(\n",
    "                input_pts, \n",
    "                t, \n",
    "                self._time, \n",
    "                self._time_out\n",
    "            ) # NOTE: deformation network\n",
    "            input_pts_orig = input_pts[:, :3] # NOTE: drop direction; but was it already dropped by torch.split?\n",
    "\n",
    "            # NOTE: re-embed with delta transformations\n",
    "            input_pts = self.embed_fn(input_pts_orig + dx) \n",
    "\n",
    "        # NOTE: infer canonical network\n",
    "        out, _ = self._occ(\n",
    "            torch.cat([input_pts, input_views], dim=-1), \n",
    "            t # NOTE: `t` is unused\n",
    "        )\n",
    "\n",
    "        return out, dx\n",
    "    \n",
    "class NeRFWrapper:\n",
    "    @staticmethod\n",
    "    def get_by_name(type, *args, **kwargs):\n",
    "        print(f\"NeRF type selected: {type}\")\n",
    "\n",
    "        if type == \"original\":\n",
    "            model = NeRFOriginal(*args, **kwargs)\n",
    "        elif type == \"direct_temporal\":\n",
    "            model = DirectTemporalNeRF(*args, **kwargs)\n",
    "        else:\n",
    "            raise ValueError(f\"Type {type} not recognized.\")\n",
    "        \n",
    "        return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: evaluate networks\n",
    "\n",
    "import NeRF.model\n",
    "\n",
    "def batchify(fn, chunk):\n",
    "\n",
    "    if chunk is None:\n",
    "        return fn\n",
    "    \n",
    "    def ret(inputs_pos, inputs_time):\n",
    "        num_batches = inputs_pos.shape[0]\n",
    "\n",
    "        out_list = []\n",
    "        dx_list = []\n",
    "        for i in range(0, num_batches, chunk):\n",
    "            out, dx = fn(inputs_pos[i:i+chunk], [inputs_time[0][i:i+chunk], inputs_time[1][i:i+chunk]])\n",
    "            out_list += [out]\n",
    "            dx_list += [dx]\n",
    "\n",
    "        return torch.cat(out_list, dim=0), torch.cat(dx_list, dim=0)\n",
    "    \n",
    "    return ret\n",
    "\n",
    "def run_network(\n",
    "    inputs, \n",
    "    viewdirs, \n",
    "    frame_time, \n",
    "    fn, \n",
    "    embed_fn, \n",
    "    embeddirs_fn, \n",
    "    embedtime_fn, \n",
    "    netchunk=1024*64, \n",
    "    embd_time_discr=True\n",
    "):\n",
    "    \n",
    "    assert len(torch.unique(frame_time)) == 1, \"Only accepts all points from same time\"\n",
    "\n",
    "\n",
    "    if embd_time_discr:\n",
    "        B, N, _ = inputs.shape\n",
    "        input_frame_time = frame_time[:, None].expand([B, N, 1])\n",
    "        input_frame_time_flat = torch.reshape(input_frame_time, [-1, 1])\n",
    "        embedded_time = embedtime_fn(input_frame_time_flat)\n",
    "        embedded_times = [embedded_time, embedded_time]\n",
    "    else:\n",
    "        assert NotImplementedError\n",
    "\n",
    "    embedded = NeRF.model.embed(inputs, viewdirs, embed_fn, embeddirs_fn)\n",
    "\n",
    "    outputs_flat, position_delta_flat = batchify(fn, netchunk)(embedded, embedded_times)\n",
    "    outputs = torch.reshape(outputs_flat, list(inputs.shape[:-1]) + [outputs_flat.shape[-1]])\n",
    "    position_delta = torch.reshape(position_delta_flat, list(inputs.shape[:-1]) + [position_delta_flat.shape[-1]])\n",
    "\n",
    "    return outputs, position_delta\n",
    "\n",
    "\n",
    "def create_D_NeRF(args):\n",
    "\n",
    "    # NOTE: `input_ch`: dimension of embedded positions\n",
    "    embed_fn, input_ch = get_embedder(args.multires, 3, args.i_embed)\n",
    "\n",
    "    input_ch_views = 0\n",
    "    embeddirs_fn = None\n",
    "    if args.use_viewdirs:\n",
    "        # NOTE: `input_ch_views`: dimension of embedded directions\n",
    "        embeddirs_fn, input_ch_views = get_embedder(args.multires_views, 3, args.i_embed)\n",
    "\n",
    "    embedtime_fn, input_ch_time = get_embedder(args.multires, 1, args.i_embed)\n",
    "\n",
    "    output_ch = 5 if args.N_importance > 0 else 4\n",
    "    skips = [4]\n",
    "    model = NeRFWrapper.get_by_name(\n",
    "        args.nerf_type, \n",
    "        D=args.netdepth, \n",
    "        W=args.netwidth, \n",
    "        input_ch=input_ch, \n",
    "        output_ch=output_ch, \n",
    "        skips=skips,\n",
    "        input_ch_views=input_ch_views,\n",
    "        input_ch_time=input_ch_time, \n",
    "        use_viewdirs=args.use_viewdirs,\n",
    "        embed_fn=embed_fn, \n",
    "        zero_canonical=not args.not_zero_canonical\n",
    "    ).to(device)\n",
    "    grad_vars = list(model.parameters())\n",
    "\n",
    "    model_fine = None\n",
    "    if args.use_two_models_for_fine:\n",
    "        model_fine = NeRFWrapper.get_by_name(\n",
    "            args.nerf_type, \n",
    "            D=args.netdepth_fine, \n",
    "            W=args.netwidth_fine,\n",
    "            input_ch=input_ch, \n",
    "            output_ch=output_ch, \n",
    "            skips=skips,\n",
    "            input_ch_views=input_ch_views,\n",
    "            input_ch_time=input_ch_time,\n",
    "            use_viewdirs=args.use_viewdirs,\n",
    "            embed_fn=embed_fn, \n",
    "            zero_canonical=not args.not_zero_canonical,\n",
    "        ).to(device)\n",
    "        grad_vars += list(model_fine.parameters())\n",
    "\n",
    "    # NOTE: helper function to running network\n",
    "    network_query_fn = lambda inputs, viewdirs, ts, network_fn: run_network(\n",
    "        inputs, \n",
    "        viewdirs, \n",
    "        ts, \n",
    "        network_fn, \n",
    "        embed_fn=embed_fn, \n",
    "        embeddirs_fn=embeddirs_fn, \n",
    "        embedtime_fn=embedtime_fn, \n",
    "        netchunk=args.netchunk, \n",
    "        embd_time_discr=args.nerf_type != \"temporal\"\n",
    "    )\n",
    "\n",
    "    # NOTE: create optimizer\n",
    "    optimizer = torch.optim.Adam(params=grad_vars, lr=args.lrate, betas=(0.9, 0.999))\n",
    "    '''\n",
    "    if args.do_half_precision:\n",
    "        print(\"Run model at half precision\")\n",
    "        if model_fine is not None:\n",
    "            [model, model_fine], optimizers = amp.initialize([model, model_fine], optimizer, opt_level='O1')\n",
    "        else:\n",
    "            model, optimizers = amp.initialize(model, optimizer, opt_level='O1')\n",
    "    '''\n",
    "\n",
    "    start = 0\n",
    "    basedir = args.basedir\n",
    "    expname = args.expname\n",
    "\n",
    "    \n",
    "    ##########################\n",
    "    # NOTE: Load checkpoints\n",
    "    if args.ft_path is not None and args.ft_path!='None':\n",
    "        ckpts = [args.ft_path]\n",
    "    else:\n",
    "        ckpts = [os.path.join(basedir, expname, f) for f in sorted(os.listdir(os.path.join(basedir, expname))) if 'tar' in f]\n",
    "\n",
    "    print('Found ckpts', ckpts)\n",
    "    if len(ckpts) > 0 and not args.no_reload:\n",
    "        ckpt_path = ckpts[-1]\n",
    "        print('Reloading from', ckpt_path)\n",
    "        ckpt = torch.load(ckpt_path)\n",
    "\n",
    "        start = ckpt['global_step']\n",
    "        optimizer.load_state_dict(ckpt['optimizer_state_dict'])\n",
    "\n",
    "        # Load model\n",
    "        model.load_state_dict(ckpt['network_fn_state_dict'])\n",
    "        if model_fine is not None:\n",
    "            model_fine.load_state_dict(ckpt['network_fine_state_dict'])\n",
    "        print(f'Loaded from {ckpt_path}')\n",
    "    ##########################\n",
    "\n",
    "    render_kwargs_train = {\n",
    "        'network_query_fn' : network_query_fn,\n",
    "        'perturb' : args.perturb,\n",
    "        'N_importance' : args.N_importance,\n",
    "        'network_fine': model_fine,\n",
    "        'N_samples' : args.N_samples,\n",
    "        'network_fn' : model,\n",
    "        'use_viewdirs' : args.use_viewdirs,\n",
    "        'white_bkgd' : args.white_bkgd,\n",
    "        'raw_noise_std' : args.raw_noise_std,\n",
    "        'use_two_models_for_fine' : args.use_two_models_for_fine,\n",
    "    }\n",
    "\n",
    "    # NDC only good for LLFF-style forward facing data\n",
    "    if args.dataset_type != 'llff' or args.no_ndc:\n",
    "        render_kwargs_train['ndc'] = False\n",
    "        render_kwargs_train['lindisp'] = args.lindisp\n",
    "\n",
    "    render_kwargs_test = {k : render_kwargs_train[k] for k in render_kwargs_train}\n",
    "    render_kwargs_test['perturb'] = False\n",
    "    render_kwargs_test['raw_noise_std'] = 0.0\n",
    "\n",
    "    return render_kwargs_train, render_kwargs_test, start, grad_vars, optimizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: rendering\n",
    "\n",
    "from typing import Dict\n",
    "import NeRF.ops\n",
    "import NeRF.rendering\n",
    "import NeRF.sampler\n",
    "\n",
    "def render_rays_D(\n",
    "    ray_batch, \n",
    "    network_fn, \n",
    "    network_query_fn, \n",
    "    N_samples, \n",
    "    retraw=False, \n",
    "    lindisp=False, \n",
    "    perturb=0.0, \n",
    "    N_importance=0, \n",
    "    network_fine=None, \n",
    "    white_bkgd=False, \n",
    "    raw_noise_std=0.0, \n",
    "    verbose=False, \n",
    "    pytest=False, \n",
    "    z_vals=None, \n",
    "    use_two_models_for_fine=False\n",
    "):\n",
    "    def __stability_check(ret: Dict):\n",
    "        if False: # TODO: add DEBUG\n",
    "            return\n",
    "        for key, tensor in ret.items():\n",
    "            if torch.isnan(tensor).any():\n",
    "                pass\n",
    "                # print(f\"[ERROR] Numerical error! {key=} contains NaN.\")\n",
    "            if torch.isinf(tensor).any():\n",
    "                pass\n",
    "                # print(f\"[ERROR] Numerical error! {key=} contains INF.\")\n",
    "\n",
    "    rays_o, rays_d, viewdirs, near, far, frame_time = NeRF.rendering.decompose_ray_batch(ray_batch, is_time_included=True)\n",
    "    N_rays = ray_batch.shape[0]\n",
    "    z_samples = None\n",
    "    rgb_map_0, disp_map_0, acc_map_0, position_delta_0 = None, None, None, None\n",
    "\n",
    "    # print(f\"[DEBUG] in render_rays_D(...), 1. before generating `z_vals`; {NeRF.util.get_gpu_memory_usage()=}\")\n",
    "\n",
    "    if z_vals is None:\n",
    "        z_vals = NeRF.rendering.sample_z(near, far, N_samples, lindisp)\n",
    "        z_vals = z_vals.expand([N_rays, N_samples])\n",
    "        z_vals = NeRF.rendering.add_noise_z(z_vals, perturb, pytest)\n",
    "\n",
    "        pts = rays_o[..., None, :] + rays_d[..., None, :] * z_vals[..., :, None]\n",
    "\n",
    "        if N_importance <= 0: # NOTE: coarse network only\n",
    "            raw, position_delta = network_query_fn(pts, viewdirs, frame_time, network_fn)\n",
    "            rgb_map, disp_map, acc_map, weights, _ = NeRF.rendering.raw2outputs(raw, z_vals, rays_d, raw_noise_std, white_bkgd, pytest)\n",
    "        else: # NOTE: coarse + fine network\n",
    "            if use_two_models_for_fine: # NOTE: UNUSED\n",
    "                raw, position_delta_0 = network_query_fn(pts, viewdirs, frame_time, network_fn)\n",
    "                rgb_map_0, disp_map_0, acc_map_0, weights, _ = NeRF.rendering.raw2outputs(raw, z_vals, rays_d, raw_noise_std, white_bkgd, pytest)\n",
    "            else:\n",
    "                # NOTE: run coarse network to get weights (samples) only\n",
    "                with torch.no_grad():\n",
    "                    raw, _ = network_query_fn(pts, viewdirs, frame_time, network_fn)\n",
    "                    _, _, _, weights, _ = NeRF.rendering.raw2outputs(raw, z_vals, rays_d, raw_noise_std, white_bkgd, pytest)\n",
    "\n",
    "            # NOTE: importance sampling\n",
    "            z_vals_mid = 0.5 * (z_vals[..., 1:] + z_vals[..., :-1])\n",
    "            z_importance_samples = NeRF.sampler.sample_pdf(\n",
    "                z_vals_mid, \n",
    "                weights[..., 1:-1], \n",
    "                N_importance, \n",
    "                det=(perturb == 0.0), \n",
    "                pytest=pytest\n",
    "            )\n",
    "            z_importance_samples = z_importance_samples.detach()\n",
    "\n",
    "            # NOTE: aggregate importance samples\n",
    "            z_vals, __IDX_NO_NEED = torch.sort(\n",
    "                torch.cat([z_vals, z_importance_samples], dim=-1),\n",
    "                dim=-1\n",
    "            )\n",
    "\n",
    "    \n",
    "    # print(f\"[DEBUG] in render_rays_D(...), 2. after generating `z_vals` and before calling `network_query_fn`; {NeRF.util.get_gpu_memory_usage()=}\")\n",
    "\n",
    "    pts = rays_o[..., None, :] + rays_d[..., None, :] * z_vals[..., :, None]\n",
    "    run_fn = network_fn if network_fine is None else network_fine\n",
    "    raw, position_delta = network_query_fn(pts, viewdirs, frame_time, run_fn)\n",
    "    \n",
    "    # print(f\"[DEBUG] in render_rays_D(...), 3. after calling `network_query_fn` and before calling `raw2outputs`; {NeRF.util.get_gpu_memory_usage()=}\")\n",
    "\n",
    "    rgb_map, disp_map, acc_map, weights, _ = NeRF.rendering.raw2outputs(raw, z_vals, rays_d, raw_noise_std, white_bkgd, pytest)\n",
    "\n",
    "    # print(f\"[DEBUG] in render_rays_D(...), 4. after calling `raw2outputs`; {NeRF.util.get_gpu_memory_usage()=}\")\n",
    "\n",
    "    ret = {'rgb_map' : rgb_map, 'disp_map' : disp_map, 'acc_map' : acc_map, 'z_vals' : z_vals,\n",
    "           'position_delta' : position_delta}\n",
    "    if retraw:\n",
    "        ret['raw'] = raw\n",
    "    if N_importance > 0:\n",
    "        if rgb_map_0 is not None:\n",
    "            ret['rgb0'] = rgb_map_0\n",
    "        if disp_map_0 is not None:\n",
    "            ret['disp0'] = disp_map_0\n",
    "        if acc_map_0 is not None:\n",
    "            ret['acc0'] = acc_map_0\n",
    "        if position_delta_0 is not None:\n",
    "            ret['position_delta_0'] = position_delta_0\n",
    "        if z_samples is not None:\n",
    "            ret['z_std'] = torch.std(z_samples, dim=-1, unbiased=False)  # [N_rays]\n",
    "\n",
    "    __stability_check(ret)\n",
    "    \n",
    "    return ret\n",
    "\n",
    "def render_path(\n",
    "    render_poses, \n",
    "    render_times, \n",
    "    hwf, \n",
    "    chunk, \n",
    "    render_kwargs, \n",
    "    gt_imgs=None, \n",
    "    savedir=None, \n",
    "    render_factor=0,\n",
    "    save_also_gt=False,\n",
    "    i_offset=0\n",
    "):\n",
    "    \n",
    "    # NOTE: similar with NeRF.rendering.render_path\n",
    "\n",
    "    H, W, focal = hwf\n",
    "    if render_factor > 0:\n",
    "        H = H // render_factor\n",
    "        W = W // render_factor\n",
    "        focal = focal / render_factor\n",
    "\n",
    "    if savedir is not None:\n",
    "        save_dir_estim = os.path.join(savedir, \"estim\")\n",
    "        save_dir_gt = os.path.join(savedir, \"gt\")\n",
    "        os.makedirs(save_dir_estim, exist_ok=True)\n",
    "        os.makedirs(save_dir_gt, exist_ok=True)\n",
    "\n",
    "    rgbs, disps, accs = [], [], []\n",
    "\n",
    "    for i, (c2w, frame_time) in enumerate(zip(tqdm(render_poses), render_times)):\n",
    "        \n",
    "        \n",
    "\n",
    "        rgb, disp, acc, _ = render(\n",
    "            H, W, focal, \n",
    "            chunk=chunk, c2w=c2w[:3, :4], \n",
    "            frame_time=frame_time, \n",
    "            **render_kwargs\n",
    "        )\n",
    "\n",
    "        rgbs.append(rgb.cpu().numpy())\n",
    "        disps.append(disp.cpu().numpy())\n",
    "        accs.append(acc.cpu().numpy())\n",
    "\n",
    "        if savedir is not None:\n",
    "            rgb8_estim = NeRF.ops.to_8b(rgbs[-1])\n",
    "            imageio.imwrite(os.path.join(save_dir_estim, f\"{i+i_offset:03d}.png\"), rgb8_estim)\n",
    "\n",
    "            if save_also_gt:\n",
    "                rgb8_gt = NeRF.ops.to_8b(gt_imgs[i])\n",
    "                imageio.imwrite(os.path.join(save_dir_gt, f\"{i+i_offset:03d}.png\"), rgb8_gt)\n",
    "\n",
    "    return np.stack(rgbs, 0), np.stack(disps, 0), np.stack(accs, 0)\n",
    "\n",
    "def render(\n",
    "    H, \n",
    "    W, \n",
    "    focal, \n",
    "    chunk=32*1024, \n",
    "    rays=None, \n",
    "    c2w=None, \n",
    "    ndc=True, \n",
    "    near=0.0, \n",
    "    far=1.0, \n",
    "    frame_time=None, \n",
    "    use_viewdirs=False, \n",
    "    c2w_staticcam=None, \n",
    "    **kwargs\n",
    "):\n",
    "    \n",
    "    K = np.array([\n",
    "        [focal, 0, 0.5 * W], \n",
    "        [0, focal, 0.5 * H], \n",
    "        [0, 0, 1]\n",
    "    ])\n",
    "\n",
    "    # print(f\"[DEBUG] in render(...), before calling `prepare_rays(...)`; {NeRF.util.get_gpu_memory_usage()=}\")\n",
    "\n",
    "    rays_o, rays_d, near, far, viewdirs, rays_original_shape = NeRF.rendering.prepare_rays(H, W, K, rays, c2w, ndc, near, far, use_viewdirs, c2w_staticcam)\n",
    "\n",
    "    # print(f\"[DEBUG] in render(...), after calling `prepare_rays(...)`; {NeRF.util.get_gpu_memory_usage()=}\")\n",
    "\n",
    "    frame_time = frame_time * torch.ones_like(rays_d[..., :1])\n",
    "    rays = torch.cat([rays_o, rays_d, near, far, frame_time], dim=-1) # TODO: wrapping function that returns `rays`, as we can concat frame_time\n",
    "    if use_viewdirs:\n",
    "        rays = torch.cat([rays, viewdirs], dim=-1)\n",
    "\n",
    "    \n",
    "    # print(f\"[DEBUG] in render(...), before calling `images_from_rendering(...)`; {NeRF.util.get_gpu_memory_usage()=}\")\n",
    "\n",
    "\n",
    "    # TODO: break down to figure out OOM\n",
    "    ret_list, ret_dict = NeRF.rendering.images_from_rendering(rays, chunk, render_rays_D, rays_original_shape, **kwargs)\n",
    "\n",
    "    \n",
    "    # print(f\"[DEBUG] in render(...), after calling `images_from_rendering(...)`; {NeRF.util.get_gpu_memory_usage()=}\")\n",
    "\n",
    "    return ret_list + [ret_dict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: training\n",
    "\n",
    "import NeRF.trainer\n",
    "\n",
    "def train(args):\n",
    "\n",
    "    # NOTE: load data\n",
    "    # NOTE: NeRF on dynamic scene: hard; until 2022 it was limited to faces e.g., HyperNeRF, NeRFace etc\n",
    "    if args.dataset_type == \"blender\":\n",
    "        images, poses, times, render_poses, render_times, hwf, i_split = load_blender_data(args.datadir, args.half_res, args.testskip)\n",
    "\n",
    "        i_train, i_val, i_test, near, far, images = NeRF.dataloader.post_load_blender_data(i_split, images, args.white_bkgd)\n",
    "\n",
    "    else:\n",
    "        raise KeyError(f\"[ERROR] unknown {args.dataset_type=}! Exiting...\")\n",
    "    \n",
    "    min_time, max_time = times[i_train[0]], times[i_train[-1]]\n",
    "    assert min_time == 0.0, f\"[ERROR] {min_time=} must start at 0\"\n",
    "    assert max_time == 1.0, f\"[ERROR] {max_time=} must be 1\"\n",
    "\n",
    "    H, W, focal = hwf\n",
    "    H, W = int(H), int(W)\n",
    "    hwf = [H, W, focal]\n",
    "    K = np.array([\n",
    "        [focal, 0, 0.5 * W], \n",
    "        [0, focal, 0.5 * H], \n",
    "        [0, 0, 1]\n",
    "    ])\n",
    "\n",
    "    \n",
    "    basedir = args.basedir\n",
    "    expname = args.expname\n",
    "    NeRF.trainer.save_args_and_config(basedir, expname, args)\n",
    "\n",
    "    if args.render_test:\n",
    "        render_poses = np.array(poses[i_test])\n",
    "        render_times = np.array(times[i_test])\n",
    "    render_poses = torch.Tensor(render_poses).to(device)\n",
    "    render_times = torch.Tensor(render_times).to(device)\n",
    "\n",
    "    # NOTE: create NeRF model\n",
    "    render_train_kwargs, render_test_kwargs, global_step, grad_vars, optimizer = NeRF.trainer.initialize_NeRF(create_D_NeRF, near, far, args)\n",
    "\n",
    "    # TODO: separate from `train`; this is only an inference function!\n",
    "    if args.render_only:\n",
    "        with torch.no_grad():\n",
    "            test_save_dir = os.path.join(basedir, expname, f\"render_only_{'test' if args.render_test else 'path'}_{global_step:06d}\")\n",
    "            os.makedirs(test_save_dir, exist_ok=True)\n",
    "\n",
    "            rgbs, _, _ = render_path(\n",
    "                render_poses, \n",
    "                render_times, \n",
    "                hwf, \n",
    "                args.chunk,\n",
    "                render_test_kwargs, \n",
    "                gt_imgs=images[i_test] if args.render_test else None,\n",
    "                savedir=test_save_dir,\n",
    "                render_factor=args.render_factor,\n",
    "                save_also_gt=False\n",
    "            )\n",
    "\n",
    "            imageio.mimwrite(\n",
    "                os.path.join(test_save_dir, \"video.mp4\"), \n",
    "                NeRF.ops.to_8b(rgbs), fps=10, quality=8)\n",
    "            print(f\"[INFO ] Done render_only at {test_save_dir}\")\n",
    "\n",
    "            return\n",
    "    \n",
    "    N_rand = args.N_rand\n",
    "    use_batching = not args.no_batching\n",
    "    if use_batching:\n",
    "        rays_rgb = NeRF.trainer.shuffle_rays(H, W, K, images, poses, i_train)\n",
    "        i_batch = 0\n",
    "\n",
    "    if use_batching:\n",
    "        rays_rgb = torch.Tensor(rays_rgb).to(device)\n",
    "    images = torch.Tensor(images).to(device)\n",
    "    poses = torch.Tensor(poses).to(device)\n",
    "    times = torch.Tensor(times).to(device)\n",
    "\n",
    "    psnrs = []\n",
    "    iternums = []\n",
    "    N_iters = args.N_iter + 1\n",
    "\n",
    "    start = global_step\n",
    "    for i in trange(start, N_iters):\n",
    "\n",
    "        print(f\"[DEBUG] iter #{global_step} - {NeRF.util.get_gpu_memory_usage()}\")\n",
    "\n",
    "        time0 = time.time()\n",
    "\n",
    "        if use_batching:\n",
    "            # TODO: batching w/ time not implemented; do later\n",
    "            raise NotImplementedError(\"Time not implemented\") \n",
    "\n",
    "            # Random over all images\n",
    "            batch = rays_rgb[i_batch:i_batch+N_rand] # [B, 2+1, 3*?]\n",
    "            batch = torch.transpose(batch, 0, 1)\n",
    "            batch_rays, target_s = batch[:2], batch[2]\n",
    "\n",
    "            i_batch += N_rand\n",
    "            if i_batch >= rays_rgb.shape[0]:\n",
    "                print(\"Shuffle data after an epoch!\")\n",
    "                rand_idx = torch.randperm(rays_rgb.shape[0])\n",
    "                rays_rgb = rays_rgb[rand_idx]\n",
    "                i_batch = 0\n",
    "\n",
    "        else: # NOTE: image batch\n",
    "            if i >= args.precrop_iters_time:\n",
    "                img_i = np.random.choice(i_train)\n",
    "            else:\n",
    "                skip_factor = i / float(args.precrop_iters_time) * len(i_train)\n",
    "                max_sample = max(int(skip_factor), 3)\n",
    "                img_i = np.random.choice(i_train[:max_sample])\n",
    "\n",
    "            target = images[img_i]\n",
    "            pose = poses[img_i, :3, :4]\n",
    "            frame_time = times[img_i]\n",
    "\n",
    "            if N_rand is not None:\n",
    "                rays_o, rays_d = NeRF.rendering.get_rays(H, W, K, torch.Tensor(pose))\n",
    "\n",
    "                # NOTE: for fast training\n",
    "                if i < args.precrop_iters:\n",
    "                    dH = int(H//2 * args.precrop_frac)\n",
    "                    dW = int(W//2 * args.precrop_frac)\n",
    "\n",
    "                    h_start = H//2 - dH\n",
    "                    h_end = H//2 + dH - 1\n",
    "                    h_steps = 2*dH \n",
    "\n",
    "                    w_start = W//2 - dW\n",
    "                    w_end = W//2 + dW - 1\n",
    "                    w_steps = 2*dW                   \n",
    "\n",
    "                    if i == start:\n",
    "                        print(f\"[INFO ] center cropping of size {2*dH} x {2*dW}is enabled until iter {args.precrop_iters}\")\n",
    "\n",
    "                # NOTE: ray batch from all pixels\n",
    "                else:\n",
    "                    \n",
    "                    h_start = 0\n",
    "                    h_end = H - 1\n",
    "                    h_steps = H\n",
    "\n",
    "                    w_start = 0\n",
    "                    w_end = W - 1\n",
    "                    w_steps = W\n",
    "                \n",
    "                coords = torch.stack(\n",
    "                    torch.meshgrid(\n",
    "                        [\n",
    "                            torch.linspace(h_start, h_end, h_steps), \n",
    "                            torch.linspace(w_start, w_end, w_steps)\n",
    "                        ], indexing=\"ij\"\n",
    "                    ), dim=-1\n",
    "                )\n",
    "\n",
    "            coords = torch.reshape(coords, [-1, 2])\n",
    "            select_coords = coords[\n",
    "                np.random.choice(coords.shape[0], size=[N_rand], replace=False)\n",
    "            ].long()\n",
    "\n",
    "            batch_rays = torch.stack([\n",
    "                rays_o[select_coords[:, 0], select_coords[:, 1]],\n",
    "                rays_d[select_coords[:, 0], select_coords[:, 1]] \n",
    "            ], dim=0)\n",
    "\n",
    "            # NOTE: GT pixels\n",
    "            target_s = target[select_coords[:, 0], select_coords[:, 1]]\n",
    "\n",
    "        # NOTE: optimization\n",
    "        rgb, disp, acc, extras = render(\n",
    "            H, W, focal, \n",
    "            chunk=args.chunk, \n",
    "            rays=batch_rays, \n",
    "            frame_time=frame_time, \n",
    "            verbose=i < 10, \n",
    "            retraw=True,\n",
    "            **render_train_kwargs\n",
    "        )\n",
    "\n",
    "        # TODO: tv loss?? (unused?)\n",
    "        # NOTE: loss calculation & backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        img_loss_fine = NeRF.loss.image_to_MSE(\n",
    "            output=rgb, \n",
    "            target=target_s\n",
    "        )\n",
    "        loss = img_loss_fine\n",
    "        if \"rgb0\" in extras:\n",
    "            img_loss_coarse = NeRF.loss.image_to_MSE(\n",
    "                output=extras[\"rgb0\"], \n",
    "                target=target_s\n",
    "            )\n",
    "            loss = loss + img_loss_coarse\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # NOTE: PSNR for statistics\n",
    "        psnr_fine = NeRF.metric.loss_to_PSNR(img_loss_fine)\n",
    "        # psnr_coarse = NeRF.metric.loss_to_PSNR(img_loss_coarse) # NOTE: `img_loss_coarse` undefined\n",
    "\n",
    "        # NOTE: learning rate update (sec. 5.3)\n",
    "        decay_rate = 0.1\n",
    "        decay_steps = args.lrate_decay * 1000\n",
    "        new_lrate = args.lrate * (decay_rate ** (global_step / decay_steps))\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group[\"lr\"] = new_lrate\n",
    "\n",
    "        # NOTE: logging\n",
    "        dt = time.time() - time0\n",
    "        # NOTE: save checkpoints TODO: rename `args.i_weights`\n",
    "        if i % args.i_weights == 0:\n",
    "            path = os.path.join(basedir, expname, f\"{i:06d}.tar\")\n",
    "            save_dict = {\n",
    "                \"global_step\": global_step, \n",
    "                \"network_fn_state_dict\": render_train_kwargs[\"network_fn\"].state_dict(), \n",
    "                \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "            }\n",
    "            if render_train_kwargs[\"network_fine\"] is not None:\n",
    "                save_dict[\"network_fine_state_dict\"] = render_train_kwargs[\"network_fine\"].state_dict()\n",
    "\n",
    "            torch.save(\n",
    "                save_dict, f=path\n",
    "            )\n",
    "\n",
    "        if i % args.i_print == 0:\n",
    "            tqdm.write(f\"[INFO ] train; iter={global_step}, loss={loss.item()}, psnr={psnr_fine.item()}\")\n",
    "\n",
    "            # NOTE: render for plot\n",
    "            img_i = np.random.choice(i_val)\n",
    "            target = images[img_i]\n",
    "            pose = poses[img_i, :3, :4]\n",
    "            frame_time = times[img_i]\n",
    "            with torch.no_grad():\n",
    "                rgb, disp, acc, extras = render(\n",
    "                    H, W, focal, \n",
    "                    chunk=args.chunk,\n",
    "                    c2w=pose, \n",
    "                    frame_time=frame_time,\n",
    "                    **render_test_kwargs\n",
    "                )\n",
    "\n",
    "            # NOTE: plotting\n",
    "            psnrs.append(psnr_fine.detach().cpu().numpy())\n",
    "            iternums.append(global_step)\n",
    "\n",
    "            fig = plt.figure(figsize=(10, 4))\n",
    "            ax1 = fig.add_subplot(1, 2, 1)\n",
    "            ax1.imshow(rgb.cpu())\n",
    "            ax1.set_title(f\"Iteration: {global_step}\")\n",
    "            ax2 = fig.add_subplot(1, 2, 2)\n",
    "            ax2.plot(iternums, psnrs)\n",
    "            ax2.set_title(f\"PSNR\")\n",
    "            fig.show() # TODO: check if this should be turned on; why not headless?\n",
    "\n",
    "            # NOTE: save plot\n",
    "            dir_iter_plot = os.path.join(basedir, expname, \"iter_plots\")\n",
    "            os.makedirs(dir_iter_plot, exist_ok=True)\n",
    "            fig.savefig(\n",
    "                os.path.join(dir_iter_plot, f\"iter={global_step}_PSNR={psnr_fine.item():.6f}.png\")\n",
    "            )\n",
    "\n",
    "            fig.clear()\n",
    "            plt.close(fig)\n",
    "\n",
    "        del loss, img_loss_fine, psnr_fine, target_s\n",
    "        if \"rgb0\" in extras:\n",
    "            del img_loss_coarse # , psnr_coarse\n",
    "        if args.add_tv_loss:\n",
    "            # del tv_loss\n",
    "            pass\n",
    "        del rgb, disp, acc, extras\n",
    "\n",
    "\n",
    "        global_step += 1\n",
    "\n",
    "    return\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: training\n",
    "\n",
    "# torch.set_default_tensor_type(\"torch.cuda.FloatTensor\")\n",
    "\n",
    "# args.chunk = 1024 * 16\n",
    "# args.netchunk = 1024 * 32\n",
    "# args.i_print = 1000\n",
    "# args.no_reload = True\n",
    "# train(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: video output\n",
    "\n",
    "\n",
    "torch.set_default_tensor_type(\"torch.cuda.FloatTensor\")\n",
    "args.chunk = 1024 // 8\n",
    "args.netchunk = 1024 // 4\n",
    "args.no_reload = False\n",
    "args.render_only = True\n",
    "args.render_test = False\n",
    "\n",
    "# print(f\"[DEBUG] in initial state; {NeRF.util.get_gpu_memory_usage()=}\")\n",
    "train(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "augmented_safeguard",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
